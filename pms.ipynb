{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin',\n",
    "                                              binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w2v_dim = 300\n",
    "w2v_vocab = len(w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "phrase = 2 * window_size + 1\n",
    "input_shape = (phrase, w2v_dim)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "\n",
    "units_per_layer = 128\n",
    "dropout_probability = 0.2\n",
    "final_activation = 'softmax'\n",
    "\n",
    "loss_function = 'categorical_crossentropy'\n",
    "\n",
    "model_file_path_string_template = 'models/{}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(clipnorm=1)\n",
    "placeholder_dense_layer = Dense(1, activation=final_activation)\n",
    "\n",
    "layers = [\n",
    "    LSTM(units_per_layer,\n",
    "         input_shape=input_shape,\n",
    "         return_sequences=True),\n",
    "    LSTM(units_per_layer,\n",
    "         input_shape=input_shape,\n",
    "         return_sequences=True),\n",
    "    LSTM(units_per_layer,\n",
    "         input_shape=input_shape,\n",
    "         dropout=dropout_probability),\n",
    "    placeholder_dense_layer\n",
    "]\n",
    "\n",
    "def generate_model():\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=[\n",
    "                      'accuracy'\n",
    "                  ])\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X, Y):\n",
    "    model.fit(X,\n",
    "              Y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def save_model_to_file(model, file_name):\n",
    "    model.save(model_file_path_string_template.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "\n",
    "def load_model_from_file(file_name):\n",
    "    model = load_model(model_file_path_string_template.format(file_name))\n",
    "    return model\n",
    "    \n",
    "def evaluate_model(model, X, Y):\n",
    "    score = model.evaluate(X,\n",
    "                           Y,\n",
    "                           batch_size=batch_size)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(lexelt):\n",
    "    return lexelt.split('.')[0]\n",
    "\n",
    "def get_pos(lexelt):\n",
    "    return lexelt.split('.')[1]\n",
    "\n",
    "def is_match_lexelt(synset, lexelt_item):\n",
    "    lexelt_with_sense = synset.name()\n",
    "    lemma_a, pos_a = get_lemma(lexelt_with_sense), get_pos(lexelt_with_sense)\n",
    "    lemma_b, pos_b = get_lemma(lexelt_item), get_pos(lexelt_item)\n",
    "    return (lemma_a.lower() == lemma_b.lower() and\n",
    "            pos_a.lower() == pos_b.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from lxml import etree\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_file_path_string_template = 'data/semeval2007/{0}/lexical-sample/english-lexical-sample.{0}.xml'\n",
    "answer_key_file_path = 'data/semeval2007/key/english-lexical-sample.test.key'\n",
    "seen_lexelts_file_name = 'lexelts.bin'\n",
    "sense_indexer_file_name = 'sense_indexer.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_or_test = 'train'\n",
    "file = data_file_path_string_template.format(train_or_test)\n",
    "root = etree.parse(file)\n",
    "\n",
    "trained_lexelt_items = set()\n",
    "lexelt_label_indexer = {} # { lexelt.item: { answer.sense_id: categorical one-hot vector index } }\n",
    "\n",
    "instance_indexer = {} # { instance_id: X.index(instance_id) and also Y.index(instance_id) }\n",
    "\n",
    "for lexelt in root.findall('lexelt'):\n",
    "    lexelt_item = lexelt.attrib['item']\n",
    "    lexelt_pos = lexelt.attrib['pos']\n",
    "    instances = lexelt.findall('instance')\n",
    "\n",
    "    number_of_instances = len(instances)\n",
    "    if number_of_instances not in range(50, 100):\n",
    "        continue\n",
    "\n",
    "    number_of_classes = len(\n",
    "        list(filter(\n",
    "            lambda synset: is_match_lexelt(synset, lexelt_item),\n",
    "            wordnet.synsets(get_lemma(lexelt_item))\n",
    "        ))\n",
    "    ) + 1 # because sense id begins from 1\n",
    "\n",
    "    print(lexelt_item, number_of_classes)\n",
    "\n",
    "    X = np.zeros((number_of_instances, phrase, w2v_dim), dtype=np.float64)\n",
    "    Y = np.zeros((number_of_instances, number_of_classes), dtype=np.uint8)\n",
    "\n",
    "    for instance_index, instance in enumerate(instances):\n",
    "        instance_id = instance.attrib['id']\n",
    "        instance_indexer[instance_id] = instance_index\n",
    "\n",
    "        answer_sense_id = instance.find('answer').attrib['senseid']\n",
    "        if lexelt_item not in lexelt_label_indexer:\n",
    "            lexelt_label_indexer[lexelt_item] = {}\n",
    "        if answer_sense_id not in lexelt_label_indexer[lexelt_item]:\n",
    "            lexelt_label_indexer[lexelt_item][answer_sense_id] = len(lexelt_label_indexer[lexelt_item]) + 1 # because sense id\n",
    "        try:\n",
    "            label_index = lexelt_label_indexer[lexelt_item][answer_sense_id]\n",
    "            Y[instance_index] = to_categorical(label_index, num_classes=number_of_classes)\n",
    "        except: # IndexError because |senses| for the lexelt in WN 3.0 < |senses| in WN 1.7 or 2.1\n",
    "            lexelt_label_indexer.pop(lexelt_item, None)\n",
    "            break\n",
    "\n",
    "        context = instance.find('context')\n",
    "        head = context.find('head').text.strip()\n",
    "        etree.strip_tags(context, 'head')\n",
    "        words = list(map(lambda sentence: word_tokenize(sentence), sent_tokenize(context.text)))\n",
    "        sentence_index, word_index = -1, -1\n",
    "        for (s_index, sentence) in enumerate(words):\n",
    "            for (w_index, word) in enumerate(sentence):\n",
    "                if word == head:\n",
    "                    sentence_index, word_index = s_index, w_index\n",
    "                    break\n",
    "        if sentence_index == -1 or word_index == -1: # Lexelt did not exist in the context\n",
    "            continue\n",
    "\n",
    "        sentence = words[sentence_index]\n",
    "        lower_bound = max(0, word_index - window_size)\n",
    "        upper_bound = min(word_index + window_size, len(sentence))\n",
    "        w2v_vectors = np.empty((phrase, w2v_dim))\n",
    "        for w_index in range(lower_bound, upper_bound):\n",
    "            word = sentence[w_index]\n",
    "            if word in w2v_model:\n",
    "                w2v_vectors[w_index - lower_bound] = w2v_model[word] # Switch reference from np.empty to word2vec vector\n",
    "        X[instance_index] = w2v_vectors\n",
    "\n",
    "    if lexelt_item not in lexelt_label_indexer:\n",
    "        continue\n",
    "\n",
    "    layers[-1] = Dense(number_of_classes,\n",
    "                       activation=final_activation)\n",
    "    model = generate_model()\n",
    "    fit_model(model, X, Y)\n",
    "    save_model_to_file(model, lexelt_item)\n",
    "    trained_lexelt_items.add(lexelt_item)\n",
    "    print(model.summary())\n",
    "    print()\n",
    "\n",
    "with open(seen_lexelts_file_name, 'wb') as l:\n",
    "    pickle.dump(trained_lexelt_items, l)\n",
    "\n",
    "with open(sense_indexer_file_name, 'wb') as s:\n",
    "    pickle.dump(lexelt_label_indexer, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test = 'test'\n",
    "file = data_file_path_string_template.format(train_or_test)\n",
    "root = etree.parse(file)\n",
    "\n",
    "with open(seen_lexelts_file_name, 'rb') as l:\n",
    "    trained_lexelt_items = pickle.load(l)\n",
    "\n",
    "with open(sense_indexer_file_name, 'rb') as s:\n",
    "    lexelt_label_indexer = pickle.load(s)\n",
    "\n",
    "answers = {}\n",
    "with open(answer_key_file_path) as k:\n",
    "    for line in k:\n",
    "        lexelt_item, instance_id, answer_sense_id = line.strip().split(' ')\n",
    "        answers[instance_id] = answer_sense_id\n",
    "\n",
    "instance_indexer = {} # { instance_id: X.index(instance_id) and also Y.index(instance_id) }\n",
    "\n",
    "for lexelt in root.findall('lexelt'):\n",
    "    lexelt_item = lexelt.attrib['item']\n",
    "    if lexelt_item not in trained_lexelt_items:\n",
    "        continue\n",
    "    lexelt_pos = lexelt.attrib['pos']\n",
    "    instances = lexelt.findall('instance')\n",
    "\n",
    "    number_of_instances = len(instances)\n",
    "\n",
    "    number_of_classes = len(\n",
    "        list(filter(\n",
    "            lambda synset: is_match_lexelt(synset, lexelt_item),\n",
    "            wordnet.synsets(get_lemma(lexelt_item))\n",
    "        ))\n",
    "    ) + 1 # because sense id begins from 1\n",
    "    # len(lexelt_label_indexer[lexelt_item]) = number of seen sense classes of lexelt_item i.e. <= number_of_classes\n",
    "\n",
    "    X = np.zeros((number_of_instances, phrase, w2v_dim), dtype=np.float64)\n",
    "    Y = np.zeros((number_of_instances, number_of_classes), dtype=np.uint8)\n",
    "\n",
    "    for instance_index, instance in enumerate(instances):\n",
    "        instance_id = instance.attrib['id']\n",
    "        instance_indexer[instance_id] = instance_index\n",
    "\n",
    "        context = instance.find('context')\n",
    "        head = context.find('head').text.strip()\n",
    "        etree.strip_tags(context, 'head')\n",
    "        words = list(map(lambda sentence: word_tokenize(sentence), sent_tokenize(context.text)))\n",
    "        sentence_index, word_index = -1, -1\n",
    "        for (s_index, sentence) in enumerate(words):\n",
    "            for (w_index, word) in enumerate(sentence):\n",
    "                if word == head:\n",
    "                    sentence_index, word_index = s_index, w_index\n",
    "                    break\n",
    "        if sentence_index == -1 or word_index == -1: # Lexelt did not exist in the context\n",
    "            continue\n",
    "\n",
    "        sentence = words[sentence_index]\n",
    "        lower_bound = max(0, word_index - window_size)\n",
    "        upper_bound = min(word_index + window_size, len(sentence))\n",
    "        w2v_vectors = np.empty((phrase, w2v_dim))\n",
    "        for w_index in range(lower_bound, upper_bound):\n",
    "            word = sentence[w_index]\n",
    "            if word in w2v_model:\n",
    "                w2v_vectors[w_index - lower_bound] = w2v_model[word] # Switch reference from np.empty to word2vec vector\n",
    "        X[instance_index] = w2v_vectors\n",
    "\n",
    "        answer_sense_id = answers[instance_id]\n",
    "        if answer_sense_id not in lexelt_label_indexer[lexelt_item]:\n",
    "            lexelt_label_indexer[lexelt_item][answer_sense_id] = len(lexelt_label_indexer[lexelt_item]) + 1 # because sense id\n",
    "        label_index = lexelt_label_indexer[lexelt_item][answer_sense_id]\n",
    "        try:\n",
    "            Y[instance_index] = to_categorical(label_index, num_classes=number_of_classes)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    model = load_model_from_file(lexelt_item)\n",
    "    score = evaluate_model(model, X, Y)\n",
    "    print(lexelt_item)\n",
    "    print(score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pms",
   "language": "python",
   "name": "pms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
