{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiayee\\documents\\gitrepositories\\pms\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin',\n",
    "                                              binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w2v_dim = 300\n",
    "w2v_vocab = len(w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "phrase = 2 * window_size + 1\n",
    "input_shape = (phrase, w2v_dim)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "\n",
    "dropout_probability = 0.2\n",
    "final_activation = 'softmax'\n",
    "\n",
    "loss_function = 'categorical_crossentropy'\n",
    "\n",
    "model_file_path_string_template = 'models/{}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiayee\\documents\\gitrepositories\\pms\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(clipnorm=1)\n",
    "placeholder_dense_layer = Dense(1,\n",
    "                                activation=final_activation)\n",
    "\n",
    "layers = [\n",
    "    LSTM(phrase,\n",
    "         input_shape=input_shape,\n",
    "         dropout=dropout_probability),\n",
    "    placeholder_dense_layer\n",
    "]\n",
    "\n",
    "'''\n",
    "layers = [\n",
    "    LSTM(phrase,\n",
    "         input_shape=input_shape,\n",
    "         return_sequences=True),\n",
    "    LSTM(phrase,\n",
    "         input_shape=input_shape,\n",
    "         return_sequences=True),\n",
    "    LSTM(phrase,\n",
    "         input_shape=input_shape,\n",
    "         dropout=dropout_probability),\n",
    "    placeholder_dense_layer\n",
    "]\n",
    "'''\n",
    "\n",
    "def generate_model():\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=[\n",
    "                      'accuracy'\n",
    "                  ])\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X, Y):\n",
    "    model.fit(X,\n",
    "              Y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def save_model_to_file(model, file_name):\n",
    "    model.save(model_file_path_string_template.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "\n",
    "def load_model_from_file(file_name):\n",
    "    model = load_model(model_file_path_string_template.format(file_name))\n",
    "    return model\n",
    "    \n",
    "def evaluate_model(model, X, Y):\n",
    "    score = model.evaluate(X,\n",
    "                           Y,\n",
    "                           batch_size=batch_size)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(lexelt):\n",
    "    return lexelt.split('.')[0]\n",
    "\n",
    "def get_pos(lexelt):\n",
    "    return lexelt.split('.')[1]\n",
    "\n",
    "def is_match_lexelt(synset, lexelt_item):\n",
    "    lexelt_with_sense = synset.name()\n",
    "    lemma_a, pos_a = get_lemma(lexelt_with_sense), get_pos(lexelt_with_sense)\n",
    "    lemma_b, pos_b = get_lemma(lexelt_item), get_pos(lexelt_item)\n",
    "    return (lemma_a.lower() == lemma_b.lower() and\n",
    "            pos_a.lower() == pos_b.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from lxml import etree\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_file_path_string_template = 'data/semeval2007/{0}/lexical-sample/english-lexical-sample.{0}.xml'\n",
    "answer_key_file_path = 'data/semeval2007/key/english-lexical-sample.test.key'\n",
    "seen_lexelts_file_name = 'lexelts.bin'\n",
    "sense_indexer_file_name = 'sense_indexer.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "announce.v 5\n",
      "Epoch 1/3\n",
      "88/88 [==============================] - 3s 32ms/step - loss: nan - acc: 0.0341\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 0s 682us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 0s 989us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 6,150\n",
      "Trainable params: 6,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "approve.v 3\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 3s 52ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 0s 816us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 0s 472us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,138\n",
      "Trainable params: 6,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "authority.n 6\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 3s 30ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 478us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 667us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 6,156\n",
      "Trainable params: 6,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "avoid.v 3\n",
      "Epoch 1/3\n",
      "55/55 [==============================] - 3s 58ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "55/55 [==============================] - 0s 740us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "55/55 [==============================] - 0s 546us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,138\n",
      "Trainable params: 6,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "base.n 13\n",
      "Epoch 1/3\n",
      "92/92 [==============================] - 4s 43ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "92/92 [==============================] - 0s 674us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "92/92 [==============================] - 0s 631us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 13)                78        \n",
      "=================================================================\n",
      "Total params: 6,198\n",
      "Trainable params: 6,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "care.v 3\n",
      "cause.v 2\n",
      "Epoch 1/3\n",
      "73/73 [==============================] - 5s 66ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "73/73 [==============================] - 0s 918us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "73/73 [==============================] - 0s 781us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 6,132\n",
      "Trainable params: 6,132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "chance.n 2\n",
      "claim.v 6\n",
      "Epoch 1/3\n",
      "54/54 [==============================] - 4s 79ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "54/54 [==============================] - 0s 712us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "54/54 [==============================] - 0s 537us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 6,156\n",
      "Trainable params: 6,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "describe.v 2\n",
      "disclose.v 2\n",
      "Epoch 1/3\n",
      "55/55 [==============================] - 5s 93ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "55/55 [==============================] - 0s 564us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "55/55 [==============================] - 0s 364us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 6,132\n",
      "Trainable params: 6,132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "enjoy.v 4\n",
      "Epoch 1/3\n",
      "56/56 [==============================] - 5s 93ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "56/56 [==============================] - 0s 770us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "56/56 [==============================] - 0s 643us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 24        \n",
      "=================================================================\n",
      "Total params: 6,144\n",
      "Trainable params: 6,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "estimate.v 2\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 4s 60ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "74/74 [==============================] - 0s 784us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "74/74 [==============================] - 0s 784us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 6,132\n",
      "Trainable params: 6,132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "exist.v 3\n",
      "Epoch 1/3\n",
      "52/52 [==============================] - 5s 96ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "52/52 [==============================] - 0s 601us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "52/52 [==============================] - 0s 323us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,138\n",
      "Trainable params: 6,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "explain.v 3\n",
      "Epoch 1/3\n",
      "85/85 [==============================] - 5s 57ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "85/85 [==============================] - 0s 552us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "85/85 [==============================] - 0s 829us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,138\n",
      "Trainable params: 6,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "join.v 5\n",
      "Epoch 1/3\n",
      "68/68 [==============================] - 5s 80ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 460us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 460us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 6,150\n",
      "Trainable params: 6,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "maintain.v 2\n",
      "prepare.v 4\n",
      "Epoch 1/3\n",
      "54/54 [==============================] - 6s 118ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "54/54 [==============================] - 0s 289us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "54/54 [==============================] - 0s 289us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 24        \n",
      "=================================================================\n",
      "Total params: 6,144\n",
      "Trainable params: 6,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "promise.v 4\n",
      "Epoch 1/3\n",
      "50/50 [==============================] - 7s 132ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 0s 625us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 0s 313us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 24        \n",
      "=================================================================\n",
      "Total params: 6,144\n",
      "Trainable params: 6,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "space.n 7\n",
      "Epoch 1/3\n",
      "67/67 [==============================] - 7s 104ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "67/67 [==============================] - 0s 700us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "67/67 [==============================] - 0s 700us/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5)                 6120      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 7)                 42        \n",
      "=================================================================\n",
      "Total params: 6,162\n",
      "Trainable params: 6,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_or_test = 'train'\n",
    "file = data_file_path_string_template.format(train_or_test)\n",
    "root = etree.parse(file)\n",
    "\n",
    "trained_lexelt_items = set()\n",
    "lexelt_label_indexer = {} # { lexelt.item: { answer.sense_id: categorical one-hot vector index } }\n",
    "\n",
    "instance_indexer = {} # { instance_id: X.index(instance_id) and also Y.index(instance_id) }\n",
    "\n",
    "for lexelt in root.findall('lexelt'):\n",
    "    lexelt_item = lexelt.attrib['item']\n",
    "    lexelt_pos = lexelt.attrib['pos']\n",
    "    instances = lexelt.findall('instance')\n",
    "\n",
    "    number_of_instances = len(instances)\n",
    "    if number_of_instances not in range(50, 100):\n",
    "        continue\n",
    "\n",
    "    number_of_classes = len(\n",
    "        list(filter(\n",
    "            lambda synset: is_match_lexelt(synset, lexelt_item),\n",
    "            wordnet.synsets(get_lemma(lexelt_item))\n",
    "        ))\n",
    "    ) + 1 # because sense id begins from 1\n",
    "\n",
    "    print(lexelt_item, number_of_classes)\n",
    "\n",
    "    X = np.zeros((number_of_instances, phrase, w2v_dim), dtype=np.float64)\n",
    "    Y = np.zeros((number_of_instances, number_of_classes), dtype=np.uint8)\n",
    "\n",
    "    for instance_index, instance in enumerate(instances):\n",
    "        instance_id = instance.attrib['id']\n",
    "        instance_indexer[instance_id] = instance_index\n",
    "\n",
    "        answer_sense_id = instance.find('answer').attrib['senseid']\n",
    "        if lexelt_item not in lexelt_label_indexer:\n",
    "            lexelt_label_indexer[lexelt_item] = {}\n",
    "        if answer_sense_id not in lexelt_label_indexer[lexelt_item]:\n",
    "            lexelt_label_indexer[lexelt_item][answer_sense_id] = len(lexelt_label_indexer[lexelt_item]) + 1 # because sense id\n",
    "        try:\n",
    "            label_index = lexelt_label_indexer[lexelt_item][answer_sense_id]\n",
    "            Y[instance_index] = to_categorical(label_index, num_classes=number_of_classes)\n",
    "        except: # IndexError because |senses| for the lexelt in WN 3.0 < |senses| in WN 1.7 or 2.1\n",
    "            lexelt_label_indexer.pop(lexelt_item, None)\n",
    "            break\n",
    "\n",
    "        context = instance.find('context')\n",
    "        head = context.find('head').text.strip()\n",
    "        etree.strip_tags(context, 'head')\n",
    "        words = list(map(lambda sentence: word_tokenize(sentence), sent_tokenize(context.text)))\n",
    "        sentence_index, word_index = -1, -1\n",
    "        for (s_index, sentence) in enumerate(words):\n",
    "            for (w_index, word) in enumerate(sentence):\n",
    "                if word == head:\n",
    "                    sentence_index, word_index = s_index, w_index\n",
    "                    break\n",
    "        if sentence_index == -1 or word_index == -1: # Lexelt did not exist in the context\n",
    "            continue\n",
    "\n",
    "        sentence = words[sentence_index]\n",
    "        lower_bound = max(0, word_index - window_size)\n",
    "        upper_bound = min(word_index + window_size, len(sentence))\n",
    "        w2v_vectors = np.empty((phrase, w2v_dim))\n",
    "        for w_index in range(lower_bound, upper_bound):\n",
    "            word = sentence[w_index]\n",
    "            if word in w2v_model:\n",
    "                w2v_vectors[w_index - lower_bound] = w2v_model[word] # Switch reference from np.empty to word2vec vector\n",
    "        X[instance_index] = w2v_vectors\n",
    "\n",
    "    if lexelt_item not in lexelt_label_indexer:\n",
    "        continue\n",
    "\n",
    "    layers[-1] = Dense(number_of_classes,\n",
    "                       activation=final_activation)\n",
    "    model = generate_model()\n",
    "    fit_model(model, X, Y)\n",
    "    save_model_to_file(model, lexelt_item)\n",
    "    trained_lexelt_items.add(lexelt_item)\n",
    "    print(model.summary())\n",
    "    print()\n",
    "\n",
    "with open(seen_lexelts_file_name, 'wb') as l:\n",
    "    pickle.dump(trained_lexelt_items, l)\n",
    "\n",
    "with open(sense_indexer_file_name, 'wb') as s:\n",
    "    pickle.dump(lexelt_label_indexer, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 99ms/step\n",
      "announce.v\n",
      "[nan, 0.0]\n",
      "\n",
      "12/12 [==============================] - 2s 194ms/step\n",
      "approve.v\n",
      "[nan, 0.0]\n",
      "\n",
      "21/21 [==============================] - 2s 115ms/step\n",
      "authority.n\n",
      "[nan, 0.0]\n",
      "\n",
      "16/16 [==============================] - 2s 156ms/step\n",
      "avoid.v\n",
      "[nan, 0.0]\n",
      "\n",
      "20/20 [==============================] - 3s 139ms/step\n",
      "base.n\n",
      "[nan, 0.0]\n",
      "\n",
      "47/47 [==============================] - 1s 31ms/step\n",
      "cause.v\n",
      "[nan, 0.0]\n",
      "\n",
      "15/15 [==============================] - 4s 263ms/step\n",
      "claim.v\n",
      "[nan, 0.0]\n",
      "\n",
      "14/14 [==============================] - 5s 368ms/step\n",
      "disclose.v\n",
      "[nan, 0.0714285746216774]\n",
      "\n",
      "14/14 [==============================] - 4s 252ms/step\n",
      "enjoy.v\n",
      "[nan, 0.0]\n",
      "\n",
      "16/16 [==============================] - 4s 259ms/step\n",
      "estimate.v\n",
      "[nan, 0.0]\n",
      "\n",
      "22/22 [==============================] - 4s 175ms/step\n",
      "exist.v\n",
      "[nan, 0.0]\n",
      "\n",
      "18/18 [==============================] - 4s 214ms/step\n",
      "explain.v\n",
      "[nan, 0.0]\n",
      "\n",
      "18/18 [==============================] - 5s 259ms/step\n",
      "join.v\n",
      "[nan, 0.0]\n",
      "\n",
      "18/18 [==============================] - 2s 105ms/step\n",
      "prepare.v\n",
      "[nan, 0.0]\n",
      "\n",
      "8/8 [==============================] - 5s 580ms/step\n",
      "promise.v\n",
      "[nan, 0.0]\n",
      "\n",
      "14/14 [==============================] - 4s 253ms/step\n",
      "space.n\n",
      "[nan, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_or_test = 'test'\n",
    "file = data_file_path_string_template.format(train_or_test)\n",
    "root = etree.parse(file)\n",
    "\n",
    "with open(seen_lexelts_file_name, 'rb') as l:\n",
    "    trained_lexelt_items = pickle.load(l)\n",
    "\n",
    "with open(sense_indexer_file_name, 'rb') as s:\n",
    "    lexelt_label_indexer = pickle.load(s)\n",
    "\n",
    "answers = {}\n",
    "with open(answer_key_file_path) as k:\n",
    "    for line in k:\n",
    "        lexelt_item, instance_id, answer_sense_id = line.strip().split(' ')\n",
    "        answers[instance_id] = answer_sense_id\n",
    "\n",
    "instance_indexer = {} # { instance_id: X.index(instance_id) and also Y.index(instance_id) }\n",
    "\n",
    "for lexelt in root.findall('lexelt'):\n",
    "    lexelt_item = lexelt.attrib['item']\n",
    "    if lexelt_item not in trained_lexelt_items:\n",
    "        continue\n",
    "    lexelt_pos = lexelt.attrib['pos']\n",
    "    instances = lexelt.findall('instance')\n",
    "\n",
    "    number_of_instances = len(instances)\n",
    "\n",
    "    number_of_classes = len(\n",
    "        list(filter(\n",
    "            lambda synset: is_match_lexelt(synset, lexelt_item),\n",
    "            wordnet.synsets(get_lemma(lexelt_item))\n",
    "        ))\n",
    "    ) + 1 # because sense id begins from 1\n",
    "    # len(lexelt_label_indexer[lexelt_item]) = number of seen sense classes of lexelt_item i.e. <= number_of_classes\n",
    "\n",
    "    X = np.zeros((number_of_instances, phrase, w2v_dim), dtype=np.float64)\n",
    "    Y = np.zeros((number_of_instances, number_of_classes), dtype=np.uint8)\n",
    "\n",
    "    for instance_index, instance in enumerate(instances):\n",
    "        instance_id = instance.attrib['id']\n",
    "        instance_indexer[instance_id] = instance_index\n",
    "\n",
    "        context = instance.find('context')\n",
    "        head = context.find('head').text.strip()\n",
    "        etree.strip_tags(context, 'head')\n",
    "        words = list(map(lambda sentence: word_tokenize(sentence), sent_tokenize(context.text)))\n",
    "        sentence_index, word_index = -1, -1\n",
    "        for (s_index, sentence) in enumerate(words):\n",
    "            for (w_index, word) in enumerate(sentence):\n",
    "                if word == head:\n",
    "                    sentence_index, word_index = s_index, w_index\n",
    "                    break\n",
    "        if sentence_index == -1 or word_index == -1: # Lexelt did not exist in the context\n",
    "            continue\n",
    "\n",
    "        sentence = words[sentence_index]\n",
    "        lower_bound = max(0, word_index - window_size)\n",
    "        upper_bound = min(word_index + window_size, len(sentence))\n",
    "        w2v_vectors = np.empty((phrase, w2v_dim))\n",
    "        for w_index in range(lower_bound, upper_bound):\n",
    "            word = sentence[w_index]\n",
    "            if word in w2v_model:\n",
    "                w2v_vectors[w_index - lower_bound] = w2v_model[word] # Switch reference from np.empty to word2vec vector\n",
    "        X[instance_index] = w2v_vectors\n",
    "\n",
    "        answer_sense_id = answers[instance_id]\n",
    "        if answer_sense_id not in lexelt_label_indexer[lexelt_item]:\n",
    "            lexelt_label_indexer[lexelt_item][answer_sense_id] = len(lexelt_label_indexer[lexelt_item]) + 1 # because sense id\n",
    "        label_index = lexelt_label_indexer[lexelt_item][answer_sense_id]\n",
    "        try:\n",
    "            Y[instance_index] = to_categorical(label_index, num_classes=number_of_classes)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    model = load_model_from_file(lexelt_item)\n",
    "    score = evaluate_model(model, X, Y)\n",
    "    print(lexelt_item)\n",
    "    print(score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pms",
   "language": "python",
   "name": "pms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
