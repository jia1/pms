{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiayee\\documents\\gitrepositories\\pms\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin',\n",
    "                                              binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w2v_vocab = len(w2v_model.vocab)\n",
    "w2v_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "phrase = 2 * window_size + 1\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiayee\\documents\\gitrepositories\\pms\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "input_length = phrase\n",
    "input_shape = (phrase, w2v_dim)\n",
    "\n",
    "dropout_probability = 0.2\n",
    "\n",
    "layers = [\n",
    "    LSTM(phrase,\n",
    "         input_shape=input_shape,\n",
    "         dropout=dropout_probability,\n",
    "         recurrent_dropout=dropout_probability,\n",
    "         return_sequences=True),\n",
    "    LSTM(phrase,\n",
    "         dropout=dropout_probability,\n",
    "         recurrent_dropout=dropout_probability,\n",
    "         input_shape=input_shape),\n",
    "    Dense(1,\n",
    "          activation='softmax')\n",
    "]\n",
    "\n",
    "def generate_model():\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    model.compile(optimizer='adam',\n",
    "                  metrics=['accuracy'],\n",
    "                  loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X, Y):\n",
    "    model.fit(X,\n",
    "              Y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def save_model_to_file(model, file_name):\n",
    "    model.save('models/{}.h5'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "\n",
    "def load_model_from_file(file_name):\n",
    "    model = load_model('models/{}.h5'.format(file_name))\n",
    "    return model\n",
    "    \n",
    "def evaluate_model(model, X, Y):\n",
    "    score = model.evaluate(X,\n",
    "                           Y,\n",
    "                           batch_size=batch_size)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(lexelt):\n",
    "    return lexelt.split('.')[0]\n",
    "\n",
    "def get_pos(lexelt):\n",
    "    return lexelt.split('.')[1]\n",
    "\n",
    "def is_match_lexelt(synset, lexelt_item):\n",
    "    lexelt_with_sense = synset.name()\n",
    "    lemma_a, pos_a = get_lemma(lexelt_with_sense), get_pos(lexelt_with_sense)\n",
    "    lemma_b, pos_b = get_lemma(lexelt_item), get_pos(lexelt_item)\n",
    "    return (lemma_a.lower() == lemma_b.lower() and\n",
    "            pos_a.lower() == pos_b.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "announce.v 5\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 5s 62ms/step - loss: nan - acc: 0.0682\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 6,370\n",
      "Trainable params: 6,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "approve.v 3\n",
      "Epoch 1/1\n",
      "53/53 [==============================] - 6s 119ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,358\n",
      "Trainable params: 6,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "authority.n 6\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 8s 92ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 6,376\n",
      "Trainable params: 6,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "avoid.v 3\n",
      "Epoch 1/1\n",
      "55/55 [==============================] - 8s 138ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,358\n",
      "Trainable params: 6,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "base.n 13\n",
      "Epoch 1/1\n",
      "92/92 [==============================] - 9s 97ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 13)                78        \n",
      "=================================================================\n",
      "Total params: 6,418\n",
      "Trainable params: 6,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "care.v 3\n",
      "cause.v 2\n",
      "Epoch 1/1\n",
      "73/73 [==============================] - 10s 131ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 6,352\n",
      "Trainable params: 6,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "chance.n 2\n",
      "claim.v 6\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 9s 174ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 6,376\n",
      "Trainable params: 6,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "describe.v 2\n",
      "disclose.v 2\n",
      "Epoch 1/1\n",
      "55/55 [==============================] - 11s 203ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 6,352\n",
      "Trainable params: 6,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "enjoy.v 4\n",
      "Epoch 1/1\n",
      "56/56 [==============================] - 12s 216ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 24        \n",
      "=================================================================\n",
      "Total params: 6,364\n",
      "Trainable params: 6,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "estimate.v 2\n",
      "Epoch 1/1\n",
      "74/74 [==============================] - 12s 161ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 6,352\n",
      "Trainable params: 6,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "exist.v 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "52/52 [==============================] - 11s 216ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,358\n",
      "Trainable params: 6,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "explain.v 3\n",
      "Epoch 1/1\n",
      "85/85 [==============================] - 12s 144ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 6,358\n",
      "Trainable params: 6,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "join.v 5\n",
      "Epoch 1/1\n",
      "68/68 [==============================] - 14s 200ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 6,370\n",
      "Trainable params: 6,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "maintain.v 2\n",
      "prepare.v 4\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 15s 276ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 24        \n",
      "=================================================================\n",
      "Total params: 6,364\n",
      "Trainable params: 6,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "promise.v 4\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 290ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 24        \n",
      "=================================================================\n",
      "Total params: 6,364\n",
      "Trainable params: 6,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "space.n 7\n",
      "Epoch 1/1\n",
      "67/67 [==============================] - 16s 234ms/step - loss: nan - acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 5)              6120      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 7)                 42        \n",
      "=================================================================\n",
      "Total params: 6,382\n",
      "Trainable params: 6,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from lxml import etree\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pickle import dump\n",
    "\n",
    "train_or_test = 'train'\n",
    "file = 'data/semeval2007/{0}/lexical-sample/english-lexical-sample.{0}.xml'.format(train_or_test)\n",
    "root = etree.parse(file)\n",
    "\n",
    "trained_lexelt_items = set()\n",
    "instance_indexer = {} # { instance_id: X.index(instance_id) and Y.index(instance_id) }\n",
    "lexelt_label_indexer = {} # { lexelt.item: { answer.sense_id: one-hot index } }\n",
    "\n",
    "for lexelt in root.findall('lexelt'):\n",
    "    lexelt_item = lexelt.attrib['item']\n",
    "    lexelt_pos = lexelt.attrib['pos']\n",
    "    instances = lexelt.findall('instance')\n",
    "\n",
    "    number_of_instances = len(instances)\n",
    "    if number_of_instances not in range(50, 100):\n",
    "        continue\n",
    "\n",
    "    number_of_classes = len(\n",
    "        list(filter(\n",
    "            lambda synset: is_match_lexelt(synset, lexelt_item),\n",
    "            wordnet.synsets(get_lemma(lexelt_item))\n",
    "        ))\n",
    "    ) + 1 # because sense id begins from 1\n",
    "\n",
    "    print(lexelt_item, number_of_classes)\n",
    "\n",
    "    X = np.zeros((number_of_instances, phrase, w2v_dim), dtype=np.float64)\n",
    "    Y = np.zeros((number_of_instances, number_of_classes), dtype=np.uint8)\n",
    "\n",
    "    for instance_index, instance in enumerate(instances):\n",
    "        instance_id = instance.attrib['id']\n",
    "        instance_indexer[instance_id] = instance_index\n",
    "\n",
    "        answer_sense_id = instance.find('answer').attrib['senseid']\n",
    "        if lexelt_item not in lexelt_label_indexer:\n",
    "            lexelt_label_indexer[lexelt_item] = {}\n",
    "        if answer_sense_id not in lexelt_label_indexer[lexelt_item]:\n",
    "            lexelt_label_indexer[lexelt_item][answer_sense_id] = len(lexelt_label_indexer[lexelt_item]) + 1 # because sense id\n",
    "        try:\n",
    "            label_index = lexelt_label_indexer[lexelt_item][answer_sense_id]\n",
    "            Y[instance_index] = to_categorical(label_index, num_classes=number_of_classes)\n",
    "        except: # IndexError because number of senses for the lexelt in WordNet 3.0 < number of senses in WordNet 1.7 or 2.1\n",
    "            lexelt_label_indexer.pop(lexelt_item, None)\n",
    "            break\n",
    "\n",
    "        context = instance.find('context')\n",
    "        head = context.find('head').text.strip()\n",
    "        etree.strip_tags(context, 'head')\n",
    "        words = list(map(lambda sentence: word_tokenize(sentence), sent_tokenize(context.text)))\n",
    "        sentence_index, word_index = -1, -1\n",
    "        for (s_index, sentence) in enumerate(words):\n",
    "            for (w_index, word) in enumerate(sentence):\n",
    "                if word == head:\n",
    "                    sentence_index, word_index = s_index, w_index\n",
    "                    break\n",
    "        if sentence_index == -1 or word_index == -1: # Lexelt did not exist in the context\n",
    "            continue\n",
    "\n",
    "        sentence = words[sentence_index]\n",
    "        lower_bound = max(0, word_index - window_size)\n",
    "        upper_bound = min(word_index + window_size, len(sentence))\n",
    "        w2v_vectors = np.empty((phrase, w2v_dim))\n",
    "        for w_index in range(lower_bound, upper_bound):\n",
    "            word = sentence[w_index]\n",
    "            if word in w2v_model:\n",
    "                w2v_vectors[w_index - lower_bound] = w2v_model[word] # Set to ref to this vector\n",
    "        X[instance_index] = w2v_vectors\n",
    "\n",
    "    if lexelt_item not in lexelt_label_indexer:\n",
    "        continue\n",
    "\n",
    "    layers[-1] = Dense(number_of_classes,\n",
    "                      activation='softmax')\n",
    "    model = generate_model()\n",
    "    fit_model(model, X, Y)\n",
    "    save_model_to_file(model, lexelt_item)\n",
    "    trained_lexelt_items.add(lexelt_item)\n",
    "    print(model.summary())\n",
    "    print()\n",
    "\n",
    "with open('lexelts.bin', 'wb') as l:\n",
    "    dump(trained_lexelt_items, l)\n",
    "\n",
    "with open('sense_id_indexer.bin', 'wb') as s:\n",
    "    dump(lexelt_label_indexer, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from lxml import etree\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pickle import load\n",
    "\n",
    "train_or_test = 'test'\n",
    "file = 'data/semeval2007/{0}/lexical-sample/english-lexical-sample.{0}.xml'.format(train_or_test)\n",
    "root = etree.parse(file)\n",
    "\n",
    "with open('lexelts.bin', 'rb') as l:\n",
    "    trained_lexelt_items = load(l)\n",
    "\n",
    "with open('sense_id_indexer.bin', 'rb') as s:\n",
    "    lexelt_label_indexer = load(s)\n",
    "\n",
    "answers = {}\n",
    "with open('data/semeval2007/key/english-lexical-sample.{}.key'.format(train_or_test)) as k:\n",
    "    for line in k:\n",
    "        lexelt_item, instance_id, answer_sense_id = line.strip().split(' ')\n",
    "        if instance_id not in instance_indexer:\n",
    "            continue\n",
    "        instance_index = instance_indexer[instance_id]\n",
    "        answers[instance_id] = answer_sense_id\n",
    "\n",
    "instance_indexer = {} # { instance_id: X.index(instance_id) and Y.index(instance_id) }\n",
    "\n",
    "for lexelt in root.findall('lexelt'):\n",
    "    lexelt_item = lexelt.attrib['item']\n",
    "    if lexelt_item not in trained_lexelt_items:\n",
    "        continue\n",
    "    lexelt_pos = lexelt.attrib['pos']\n",
    "    instances = lexelt.findall('instance')\n",
    "\n",
    "    number_of_instances = len(instances)\n",
    "    if number_of_instances not in range(50, 100):\n",
    "        continue\n",
    "\n",
    "    number_of_classes = len(lexelt_label_indexer[lexelt_item]) + 1 # because sense id begins from 1\n",
    "\n",
    "    X = np.zeros((number_of_instances, phrase, w2v_dim), dtype=np.float64)\n",
    "    Y = np.zeros((number_of_instances, number_of_classes), dtype=np.uint8)\n",
    "\n",
    "    for instance_index, instance in enumerate(instances):\n",
    "        instance_id = instance.attrib['id']\n",
    "        instance_indexer[instance_id] = instance_index\n",
    "\n",
    "        context = instance.find('context')\n",
    "        head = context.find('head').text.strip()\n",
    "        etree.strip_tags(context, 'head')\n",
    "        words = list(map(lambda sentence: word_tokenize(sentence), sent_tokenize(context.text)))\n",
    "        sentence_index, word_index = -1, -1\n",
    "        for (s_index, sentence) in enumerate(words):\n",
    "            for (w_index, word) in enumerate(sentence):\n",
    "                if word == head:\n",
    "                    sentence_index, word_index = s_index, w_index\n",
    "                    break\n",
    "        if sentence_index == -1 or word_index == -1: # Lexelt did not exist in the context\n",
    "            continue\n",
    "\n",
    "        sentence = words[sentence_index]\n",
    "        lower_bound = max(0, word_index - window_size)\n",
    "        upper_bound = min(word_index + window_size, len(sentence))\n",
    "        w2v_vectors = np.empty((phrase, w2v_dim))\n",
    "        for w_index in range(lower_bound, upper_bound):\n",
    "            word = sentence[w_index]\n",
    "            if word in w2v_model:\n",
    "                w2v_vectors[w_index - lower_bound] = w2v_model[word] # Set to ref to this vector\n",
    "        X[instance_index] = w2v_vectors\n",
    "\n",
    "        answer_sense_id = answers[instance_id]\n",
    "        label_index = lexelt_label_indexer[lexelt_item][answer_sense_id]\n",
    "        Y[instance_index] = to_categorical(label_index, num_classes=number_of_classes)\n",
    "\n",
    "    model = load_model_from_file(lexelt_item)\n",
    "    score = evaluate_model(model, X, Y)\n",
    "    print(lexelt_item)\n",
    "    print(score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pms",
   "language": "python",
   "name": "pms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
